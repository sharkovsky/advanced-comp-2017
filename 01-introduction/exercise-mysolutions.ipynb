{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Work on this before the next lecture on 10 April. We will talk about questions, comments, and solutions during the exercise after the second lecture.\n",
    "\n",
    "Please do form study groups! When you do, make sure you can explain everything in your own words, do not simply copy&paste from others.\n",
    "\n",
    "The solutions to a lot of these problems can probably be found with Google. Please don't. You will not learn a lot by copy&pasting from the internet.\n",
    "\n",
    "If you want to get credit/examination on this course please upload your work to **your GitHub repository** for this course **before** the next lecture starts and post a link to your repository [in this thread](https://github.com/wildtreetech/advanced-comp-2017/issues/1). If you worked on things together with others please add their names to the notebook so we can see who formed groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Objective\n",
    "\n",
    "There are two objectives for this set of exercises:\n",
    "\n",
    "* get you started using python, scikit-learn, matplotlib, and GitHub. You will be using them a lot during the course, so make sure you get a good foundation to build on.\n",
    "\n",
    "* working through the steps of opening a new dataset, plotting the data, fitting a model to it, evaluating your model, and deciding on model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 0\n",
    "\n",
    "Install python, scikit-learn (v0.18), matplotlib, jupyter and git.\n",
    "\n",
    "Instructions for doing so: https://github.com/wildtreetech/advanced-comp-2017/blob/master/install.md\n",
    "\n",
    "Documentation and guides for the various tools:\n",
    "\n",
    "* [jupyter quickstart](http://jupyter.readthedocs.io/en/latest/content-quickstart.html)\n",
    "* [try jupyter without installing anything](https://try.jupyter.org/)\n",
    "* [matplotlib homepage](http://matplotlib.org/)\n",
    "* [matplotlib gallery](http://matplotlib.org/gallery.html)\n",
    "* [scikit-learn homepage](http://scikit-learn.org/stable/)\n",
    "* [scikit-learn examples](http://scikit-learn.org/stable/auto_examples/index.html)\n",
    "* [scikit-learn documentation](http://scikit-learn.org/stable/documentation.html)\n",
    "* [try git online without installing anything](https://try.github.io/levels/1/challenges/1)\n",
    "\n",
    "\n",
    "### GitHub and git\n",
    "\n",
    "* [Create a GitHub account]() for yourself or use one you already have.\n",
    "* Follow the guide on [creating a new repository](https://help.github.com/articles/create-a-repo/). Name the repository \"advanced-comp-2017\".\n",
    "\n",
    "Read up on `git clone`, `git pull`, `git push`, `git add` and `git commit`. Once you master these five commands you should be good for this course. There is a whole universe of complex things that `git` can do for you, don't worry about them for now. Once you feel comfortable with the basics you can always step it up later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "These are some useful default imports for plotting and [`numpy`](http://www.numpy.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 1\n",
    "\n",
    "In the lecture we used the nearest neighbour classifier to classify points from a toy dataset into either \"red\" or \"blue\" classes. We investigated how the performance changes as a function of model complexity and what this means for the performance of our classifier on unseen data. Instead of using a linear model as in the lecture, use a k-nearest neighbour model.\n",
    "\n",
    "* plot your dataset\n",
    "* split your dataset into a training and testing set. Comment on how you decided to split your data.\n",
    "* evaluate the performance of the classifier on your training dataset.\n",
    "* evaluate the performance of the classifier on your testing dataset.\n",
    "* repeat the above two steps for varying splits (10-90, 20-80, 30-70, ...) and comment\n",
    "  on what you see. Is there a \"best\" way to split your data?\n",
    "* comment on why the two performance estimates agree or disagree.\n",
    "* plot the accuracy of the classifier as a function of `n_neighbors`.\n",
    "* comment on the similarities and differences between the performance on the testing and training dataset.\n",
    "* is a KNeighbor Classifier with 4 or 10 neighbors more complicated?\n",
    "* find the best setting of `n_neighbors` for this dataset.\n",
    "* why is this the best setting?\n",
    "\n",
    "Use `make_blobs(n_samples=400, centers=23, random_state=42)` to create a simple dataset and use the `KNeighborsClassifier` classifier to answer the above questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "labels = [\"b\", \"r\"]\n",
    "X, y = make_blobs(n_samples=400, centers=23, random_state=42)\n",
    "y = np.take(labels, (y < 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### plot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X[y==labels[0],0],X[y==labels[0],1],'*b')\n",
    "plt.plot(X[y==labels[1],0],X[y==labels[1],1],'*r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Split into training and validation data\n",
    "\n",
    "A procedure to avoid bias is to randomly shuffle the data, then pick a subset. I will use 20% of data for validation, which I believe is a decent compromise between having enough data for validation and having enough data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def train_test_split(X,y,train_size):\n",
    "    \n",
    "    XX = np.c_[X,y]\n",
    "    np.random.shuffle(XX)\n",
    "\n",
    "    X_train = XX[:int(train_size*X.shape[0]),:-1]\n",
    "    y_train = XX[:int(train_size*X.shape[0]),-1]\n",
    "    X_val   = XX[X_train.shape[0]:,:-1]\n",
    "    y_val   = XX[X_train.shape[0]:,-1]\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,0.8)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make sure everything was ok\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(X[y==labels[0],0],X[y==labels[0],1],'*b')\n",
    "plt.plot(X[y==labels[1],0],X[y==labels[1],1],'*r')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(X_train[y_train==labels[0],0],X_train[y_train==labels[0],1],'*b')\n",
    "plt.plot(X_train[y_train==labels[1],0],X_train[y_train==labels[1],1],'*r')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(X_val[y_val==labels[0],0],X_val[y_val==labels[0],1],'*b')\n",
    "plt.plot(X_val[y_val==labels[1],0],X_val[y_val==labels[1],1],'*r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### evaluate the performance of the classifier on training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracies_test = []\n",
    "accuracies_train = []\n",
    "ks = np.arange(1, 25, 1)\n",
    "\n",
    "for n in range(50):\n",
    "    X, y = make_blobs(n_samples=400, centers=23, random_state=42+n)\n",
    "    y = np.take(labels, (y < 10))\n",
    "    X_train,X_val, y_train,y_val = train_test_split(X, y, train_size=0.5)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for k in ks:\n",
    "        clf = KNeighborsClassifier(n_neighbors=k)\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_scores.append(clf.score(X_train, y_train))\n",
    "        test_scores.append(clf.score(X_val, y_val))\n",
    "        \n",
    "    accuracies_test.append(test_scores)\n",
    "    accuracies_train.append(train_scores)\n",
    "    \n",
    "    plt.plot(ks, train_scores, c='b', alpha=0.1)\n",
    "    plt.plot(ks, test_scores, c='r', alpha=0.1)\n",
    "    \n",
    "plt.plot(ks, np.array(accuracies_test).mean(axis=0), label='Test', c='r', lw=4)\n",
    "plt.plot(ks, np.array(accuracies_train).mean(axis=0), label='Train', c='b', lw=4)\n",
    "plt.xlabel('k or inverse model complexity')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.xlim((0, max(ks)))\n",
    "plt.ylim((0.5, 1.));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracies_test = []\n",
    "accuracies_train = []\n",
    "ks = np.arange(1, 25, 1)\n",
    "\n",
    "for idx, training_size in enumerate([0.9, 0.8, 0.7, 0.6]):\n",
    "\n",
    "    plt.subplot(2,2,idx+1)\n",
    "    \n",
    "    for n in range(50):\n",
    "        X, y = make_blobs(n_samples=400, centers=23, random_state=42+n)\n",
    "        y = np.take(labels, (y < 10))\n",
    "        X_train,X_val, y_train,y_val = train_test_split(X, y, train_size=training_size)\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        for k in ks:\n",
    "            clf = KNeighborsClassifier(n_neighbors=k)\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_scores.append(clf.score(X_train, y_train))\n",
    "            test_scores.append(clf.score(X_val, y_val))\n",
    "\n",
    "        accuracies_test.append(test_scores)\n",
    "        accuracies_train.append(train_scores)\n",
    "\n",
    "        plt.plot(ks, train_scores, c='b', alpha=0.1)\n",
    "        plt.plot(ks, test_scores, c='r', alpha=0.1)\n",
    "\n",
    "\n",
    "    plt.plot(ks, np.array(accuracies_test).mean(axis=0), label='Test', c='r', lw=4)\n",
    "    plt.plot(ks, np.array(accuracies_train).mean(axis=0), label='Train', c='b', lw=4)\n",
    "    plt.xlabel('k or inverse model complexity')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlim((0, max(ks)))\n",
    "    plt.ylim((0.5, 1.));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Discussion\n",
    "\n",
    "Training and Testing errors are almost always different, and quite often the testing error is larger compared to the training error.\n",
    "\n",
    "In any case, a large difference between training and testing errors is not a good sign w.r.t the goodness of the model. \n",
    "- If the training error is very small but the testing error large,we are probably in a region of overfitting. We can try to simplify the model, or dedicate a larger chunk of the data to validation.\n",
    "- If the training error is very large, on the other hand, it may be that we are in a region of underfitting. We might require a more complex model, or a more different model altogether.\n",
    "\n",
    "It is obviously difficult to define a 'perfect' proportion for splitting training and validation data. In this particular example, we see that pretty large training datasets (90-10 or 80-20) appear to lead to better results for both training and validation errors. However, I don't know how general this rule can be.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Complexity of the model\n",
    "\n",
    "The KNearestNeighbor classifier uses the $k$ closest neighbours of a point to predict its value. To understand how this relates to complexity, I find it easier to think of two extremes:\n",
    "1. $k == dataset size$: in this case the value predicted at any point would be equal to the average of all the known values in the dataset. This corresponds to the __simplest__ model possible;\n",
    "2. $k=1$: in this case, the value predicted is equal to the value of the closes neighbor. This corresponds to the __most complex__ model, in the sense that it predicts piecewise constant function with potentially the largest number of discontinuities.\n",
    "\n",
    "In this sense, a model with $k=4$ is more complex than a model with $k=10$.\n",
    "\n",
    "### Data splitting\n",
    "\n",
    "To find the best value of k for this dataset, I refer to the plots above, and in particular the one relating to the 80-20 split (the top right plot), which appears to give best accuracy overall.\n",
    "\n",
    "### Best setting\n",
    "\n",
    "Very large values of k are correlated to small values of the training error. However, there seems to be an inverted U shape for the testing error, which reaches its maximum accuracy (for validation error) roughly around k=15. However, there is also a question of model complexity to take into account: the inverted U shape we see is actually quite flat, so in principle one could achieve comparable performance with a much smaller value of k, e.g. around 10 or even a bit less. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Question 2\n",
    "\n",
    "This is a regression problem. It mostly follows the setup of the classification problem so you should be able to reuse some of your work.\n",
    "\n",
    "* plot your dataset\n",
    "* fit a kNN regressor with varying number of `n_neighbors` and compare each regressors predictions to the location of the training and testing points. \n",
    "* plot the mean squared error of the classifier as a function of `n_neighbors` for both training and testing datasets.\n",
    "* comment on the similarities and differences between the performance on the testing and training dataset.\n",
    "* find the best setting of `n_neighbors` for this dataset.\n",
    "* why is this the best setting?\n",
    "* can you explain why the mean square error on the training dataset plateaus between ~`n_neihgors`=5 to 15 at the value that it does?\n",
    "\n",
    "Use `make_regression()` to create the dataset and use `KNeighborsRegressor` to answer the above questions. Take a look at scikit-learn's [`metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) module to compute the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_regression(n_samples=100, noise_level=0.8, random_state=2):\n",
    "    rng = check_random_state(random_state)\n",
    "    X = np.linspace(-2, 2, n_samples)\n",
    "    y = 2.0 * X + np.sin(5 * X) + rng.randn(n_samples) * noise_level\n",
    "    #y = 2.0*X + rng.randn(n_samples) * noise_level\n",
    "    #y = rng.randn(n_samples) * noise_level\n",
    "    \n",
    "    return X.reshape(-1, 1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(100,0.8,42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X,y,'*')\n",
    "X_, y_ = make_regression(400,0.0,42)\n",
    "plt.plot(X_, y_, '-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(400,0.8,2)\n",
    "X_train,X_val, y_train,y_val = train_test_split(X, y, train_size=0.8)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "rgr = KNeighborsRegressor(n_neighbors=1)\n",
    "rgr.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(X_train,y_train,'b*')\n",
    "plt.plot(X_val,y_val,'*r')\n",
    "plt.plot(X_val, rgr.predict(X_val), 'k*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "accuracies_test = []\n",
    "accuracies_train = []\n",
    "ks = np.arange(1, 35, 1)\n",
    "\n",
    "for n in range(50):\n",
    "    X, y = make_regression(100,0.8,2)\n",
    "    X_train,X_val, y_train,y_val = train_test_split(X, y, train_size=0.8)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for k in ks:\n",
    "        rgr = KNeighborsRegressor(n_neighbors=k)\n",
    "        rgr.fit(X_train, y_train)\n",
    "        train_scores.append( mean_squared_error(y_train, rgr.predict(X_train)) )\n",
    "        test_scores.append( mean_squared_error(y_val, rgr.predict(X_val)) )\n",
    "        \n",
    "    accuracies_test.append(test_scores)\n",
    "    accuracies_train.append(train_scores)\n",
    "    \n",
    "    plt.plot(ks, train_scores, c='b', alpha=0.1)\n",
    "    plt.plot(ks, test_scores, c='r', alpha=0.1)\n",
    "    \n",
    "plt.plot(ks, np.array(accuracies_train).mean(axis=0), '*-', label='Train', c='b', lw=4, alpha=0.8)\n",
    "plt.plot(ks, np.array(accuracies_test).mean(axis=0), '*-', label='Test', c='r', lw=4, alpha=0.8)\n",
    "plt.xlabel('k or inverse model complexity')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='best')\n",
    "plt.xlim((0, max(ks)))\n",
    "plt.ylim((-0.05, np.max(np.array(accuracies_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Discussion\n",
    "\n",
    "\n",
    "##### Reason for plateau\n",
    "For intermediate values of $k$ ($5 < k < 15$), we notice a plateau in the training error close to a value of $0.6$. Given that we defined a noise level of $\\sigma = 0.8$, it is tempting to associate the plateau value to $\\sigma^2$.\n",
    "The hypothesis is therefore that, for those special values of $k$, the training error approximates the noise in the data. It is reasonable to think that, since our target function $y = 2X + sin(5X)$ does not have very large derivatives, it can be approximated locally to a constant function. We confirm this hypothesis by trying to perform regression on a constant function (i.e. only noise), code below. Indeed, for increasing $k$ the training error settles on a value equal to $\\sigma^2$.\n",
    "We conclude that the plateau in the training error is given by two factors:\n",
    "1. $k$ is small enough, so that the considered neighbours span a small enough length that allows to consider the target function *almost constant*\n",
    "2. $k$ is large enough that the training error settles on the value of the noise in the data.\n",
    "\n",
    "This can be seen as a consequence of the error decomposition:\n",
    "$ error = bias^2 + var^2 + \\sigma^2 $\n",
    "where for $5 < k < 15$ we have minimal bias and variance on the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_regression(n_samples=100, noise_level=0.8, random_state=2):\n",
    "    rng = check_random_state(random_state)\n",
    "    X = np.linspace(-2, 2, n_samples)\n",
    "    #y = 2 * X + np.sin(5 * X) + rng.randn(n_samples) * noise_level\n",
    "    y = rng.randn(n_samples) * noise_level\n",
    "    \n",
    "    \n",
    "    return X.reshape(-1, 1), y\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "accuracies_test = []\n",
    "accuracies_train = []\n",
    "ks = np.arange(1, 20, 1)\n",
    "\n",
    "for n in range(50):\n",
    "    X, y = make_regression(100,0.8,2)\n",
    "    X_train,X_val, y_train,y_val = train_test_split(X, y, train_size=0.8)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for k in ks:\n",
    "        rgr = KNeighborsRegressor(n_neighbors=k)\n",
    "        rgr.fit(X_train, y_train)\n",
    "        train_scores.append( mean_squared_error(y_train, rgr.predict(X_train)) )\n",
    "        test_scores.append( mean_squared_error(y_val, rgr.predict(X_val)) )\n",
    "        \n",
    "    accuracies_test.append(test_scores)\n",
    "    accuracies_train.append(train_scores)\n",
    "    \n",
    "    plt.plot(ks, train_scores, c='b', alpha=0.1)\n",
    "    plt.plot(ks, test_scores, c='r', alpha=0.1)\n",
    "    \n",
    "plt.plot(ks, np.array(accuracies_train).mean(axis=0), '*-', label='Train', c='b', lw=4, alpha=0.8)\n",
    "plt.plot(ks, np.array(accuracies_test).mean(axis=0), '*-', label='Test', c='r', lw=4, alpha=0.8)\n",
    "plt.xlabel('k or inverse model complexity')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='best')\n",
    "plt.xlim((0, max(ks)))\n",
    "plt.ylim((-0.05, np.max(np.array(accuracies_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Question 3\n",
    "\n",
    "Logistic regression. Use a linear model to solve a two class classification problem.\n",
    "\n",
    "* What is the difference between a linear regression model and a logistic regression model?\n",
    "* plot your data and split it into a training and test set\n",
    "* draw your guess for where the decision boundary will be on the plot. Why did you pick this one?\n",
    "* use the `LogisticRegression` classifier to fit a model to your training data\n",
    "* extract the fitted coefficients from the model and draw the fitted decision boundary\n",
    "* create a function to draw the decision surface (the classifier's prediction for every point in space)\n",
    "* why is the boundary where it is?\n",
    "* **(bonus)** create new datasets with increasingly larger amounts of noise (increase the `cluster_std` argument) and plot the decision boundary for each case. What happens and why?\n",
    "* create 20 new datasets by changing the `random_state` parameter and fit a model to each. Visualise the variation in the fitted parameters and the decision boundaries you obtain. Is this a high or low variance model?\n",
    "\n",
    "Use `make_two_blobs()` to create a simple dataset and use the `LogisticRegression` classifier to answer the above questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def make_two_blobs(n_samples=400, cluster_std=2., random_state=42):\n",
    "    rng = check_random_state(random_state)\n",
    "    X = rng.multivariate_normal([5,0], [[cluster_std**2, 0], [0., cluster_std**2]],\n",
    "                                size=n_samples//2)\n",
    "    \n",
    "    X2 = rng.multivariate_normal([0, 5.], [[cluster_std**2, 0], [0., cluster_std**2]],\n",
    "                                 size=n_samples//2)\n",
    "    X = np.vstack((X, X2))\n",
    "    return X, np.hstack((np.ones(n_samples//2), np.zeros(n_samples//2)))\n",
    "\n",
    "X, y = make_two_blobs()\n",
    "labels = ['b', 'r']\n",
    "y = np.take(labels, (y < 0.5))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "is a technique that gives a model for predicting an output $y$ as a function of an input $y = f(x)$. \n",
    "The difference with regular regression is that $y$ is, in logistic regression, a categorical variable, namely $y \\in \\{ 0,1 \\}$, whereas in regular regression $y$ is a real variable $y \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X[y==labels[0],0],X[y==labels[0],1],'*b')\n",
    "plt.plot(X[y==labels[1],0],X[y==labels[1],1],'*r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(X,y,train_size):\n",
    "    \n",
    "    XX = np.c_[X,y]\n",
    "    np.random.shuffle(XX)\n",
    "\n",
    "    X_train = XX[:int(train_size*X.shape[0]),:-1]\n",
    "    y_train = XX[:int(train_size*X.shape[0]),-1]\n",
    "    X_val   = XX[X_train.shape[0]:,:-1]\n",
    "    y_val   = XX[X_train.shape[0]:,-1]\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train,X_val, y_train,y_val = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### my guess for the decision boundary\n",
    "\n",
    "was found visually, trying to put a line that would minimise the number of misplaced points. Also, I was tempted by a sort of *corridor of white* between the two blobs, which made me think the decision boundary should pass through there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X[y==labels[0],0],X[y==labels[0],1],'*b')\n",
    "plt.plot(X[y==labels[1],0],X[y==labels[1],1],'*r')\n",
    "xx = np.linspace(start=-6.0, stop=10.0, num=200)\n",
    "guessed_dec_boundary= 2.6/3.0*xx\n",
    "plt.plot(xx,guessed_dec_boundary,'-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ls = LogisticRegression()\n",
    "fit = ls.fit(X,y)\n",
    "print(fit.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X[y==labels[0],0],X[y==labels[0],1],'*b')\n",
    "plt.plot(X[y==labels[1],0],X[y==labels[1],1],'*r')\n",
    "\n",
    "xx = np.linspace(start=-6.0, stop=10.0, num=200)\n",
    "\n",
    "guessed_dec_boundary= 2.6/3.0*xx\n",
    "plt.plot(xx,guessed_dec_boundary,'--k', alpha=0.3)\n",
    "\n",
    "actual_decision_boundary = - fit.coef_[0][0]*xx / fit.coef_[0][1]\n",
    "plt.plot(xx,actual_decision_boundary,'-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Drawing the decision surface\n",
    "\n",
    "The position of the decision surface is determined by how we convert the linear model into a class probability (and obviously also by the linearity of the underlying model). \n",
    "In particular, logistic regression aims at minimizing the cost function:\n",
    "\n",
    "$ L(\\mathbf{w}) =  \\sum_n log( 1 + e^{ \\mathbf{x}^T_n\\mathbf{w} } ) - y_n \\mathbf{x}^T_n\\mathbf{w} $\n",
    "\n",
    "The decision boundary is therefore in this particular position because it is the linear function that minimizes $L(\\mathbf{w})$.\n",
    "\n",
    "It should be noted, for example, that it doesn't explicitly minimize the total number of misclassified points, because in $L(\\mathbf{w})$ not all misclassifications have the same importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate_decision(fit, xx, yy):\n",
    "    assert xx.shape == yy.shape\n",
    "    print(xx.shape)\n",
    "    zz = np.zeros(shape=xx.shape)\n",
    "    coefs = fit.coef_[0]\n",
    "    for row in range(xx.shape[0]):\n",
    "        for col in range(xx.shape[1]):\n",
    "            zz[row][col] = ( np.inner( np.array([xx[0][row], yy[col][0]]), coefs) < 0 ) \n",
    "            \n",
    "    return zz\n",
    "        \n",
    "x_space = np.linspace(start=np.min(X[:,0]), stop=np.max(X[:,0]), num=400)\n",
    "y_space = np.linspace(start=np.min(X[:,1]), stop=np.max(X[:,1]), num=400)\n",
    "xx,yy = np.meshgrid(x_space, y_space)   \n",
    "    \n",
    "def plot_decision_surface(datapoints, fit, ax):\n",
    "    X = datapoints[:,0]\n",
    "    Y = datapoints[:,1]\n",
    "    x_space = np.linspace(start=np.min(X), stop=np.max(X), num=400)\n",
    "    y_space = np.linspace(start=np.min(Y), stop=np.max(Y), num=400)\n",
    "    xx,yy = np.meshgrid(x_space, y_space)\n",
    "    zz = evaluate_decision(fit, xx, yy)\n",
    "    zz.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, zz, alpha=0.6, interp='None', cmap=plt.cm.RdBu_r)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plot_decision_surface(X,fit,ax)\n",
    "ax.plot(X[y==labels[0],0],X[y==labels[0],1],'*b')\n",
    "ax.plot(X[y==labels[1],0],X[y==labels[1],1],'*r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "As the level of noise increases, the decision surface retains the same shape. This is probably because the centroids of the blobs actually didn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_iter = 4\n",
    "noise = 1.\n",
    "labels = ['b', 'r']\n",
    "ls = LogisticRegression()\n",
    "fig = plt.figure()\n",
    "    \n",
    "def plot_decision_surface2(datapoints, fit, ax, xmin, xmax, ymin, ymax):\n",
    "    X = datapoints[:,0]\n",
    "    Y = datapoints[:,1]\n",
    "    x_space = np.linspace(start=xmin, stop=xmax, num=400)\n",
    "    y_space = np.linspace(start=ymin, stop=ymax, num=400)\n",
    "    xx,yy = np.meshgrid(x_space, y_space)\n",
    "    zz = evaluate_decision(fit, xx, yy)\n",
    "    zz.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, zz, alpha=0.6, interp='None', cmap=plt.cm.RdBu_r)\n",
    "    \n",
    "for iter in range(num_iter):\n",
    "    noise *= 1.5\n",
    "\n",
    "    X, y = make_two_blobs(n_samples=400, cluster_std=noise, random_state=42)\n",
    "    y = np.take(labels, (y < 0.5))\n",
    "    \n",
    "    fit = ls.fit(X,y)\n",
    "\n",
    "    ax = fig.add_subplot(2,2,iter+1)\n",
    "\n",
    "    plot_decision_surface2(X,fit,ax, -10, 10, -10, 10)\n",
    "    ax.plot(X[y==labels[0],0],X[y==labels[0],1],'*b')\n",
    "    ax.plot(X[y==labels[1],0],X[y==labels[1],1],'*r')\n",
    "    ax.set_xlim([-10,10])\n",
    "    ax.set_ylim([-10,10])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different random seeds\n",
    "\n",
    "I am kind of surprised that using different random seeds produces exactly the same coefficients, up to many decimal places!\n",
    "This is definitely a low-variance method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "num_iter = 20\n",
    "labels = ['b', 'r']\n",
    "ls = LogisticRegression()\n",
    "fig = plt.figure()\n",
    "    \n",
    "fits = []\n",
    "    \n",
    "random.seed(42)\n",
    "\n",
    "for iter in range(num_iter):\n",
    "    rnd_seed = random.randint(1, 230)\n",
    "    \n",
    "    X, y = make_two_blobs(n_samples=200, cluster_std=2., random_state=rnd_seed)\n",
    "    y = np.take(labels, (y < 0.5))\n",
    "    \n",
    "    fit = ls.fit(X,y)\n",
    "    fits.append(fit)\n",
    "\n",
    "    ax = fig.add_subplot(5,4,iter+1)\n",
    "\n",
    "    plot_decision_surface(X,fit,ax)\n",
    "    ax.plot(X[y==labels[0],0],X[y==labels[0],1],'*b')\n",
    "    ax.plot(X[y==labels[1],0],X[y==labels[1],1],'*r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "xcoefs = [ f.coef_[0][0] for f in fits ]\n",
    "ycoefs = [ f.coef_[0][1] for f in fits ]\n",
    "\n",
    "print(xcoefs)\n",
    "\n",
    "plt.scatter(xcoefs, ycoefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "Logistic regression. Use a more complex linear model to create a two class classifier for the \"circle inside a circle\" problem. Think about how you can increase the complexity of a logistic regression model. Visualise the classificatio naccuracy as a function of the model complexity.\n",
    "\n",
    "Use `make_circles(n_samples=400, factor=.3, noise=.1)` to create a simple dataset and use the `LogisticRegression` classifier to answer the above question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.1)\n",
    "labels = ['b', 'r']\n",
    "y = np.take(labels, (y < 0.5))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_coordinates(X):\n",
    "    Z = np.zeros(shape=X.shape)\n",
    "    for idx,coord in enumerate(X):\n",
    "        x = coord[0]\n",
    "        y = coord[1]\n",
    "        rho = np.sqrt(x**2 + y**2)\n",
    "        theta = np.arctan2(y,x)\n",
    "        Z[idx] = [rho, theta]\n",
    "        \n",
    "    return Z\n",
    "\n",
    "Z = transform_coordinates(X)\n",
    "plt.scatter(Z[:,0], Z[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ls = LogisticRegression()\n",
    "fit = ls.fit(Z,y)\n",
    "print(fit.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plot_decision_surface(Z,fit,ax)\n",
    "plt.scatter(Z[:,0], Z[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Completely separable\n",
    "\n",
    "Logistic regression cannot converge if the data, as is the case after transformations, is completely separable (see for example [this Quora answer](https://www.quora.com/Why-does-logistic-regression-not-converge-when-the-data-is-completely-separable). A simple way to look at this is to think: how would the algorithm decide between two decision boundaries that are both able to split the data with 100% accuracy, but are slightly different?\n",
    "\n",
    "A simple way around this would be to generate a lot of data, until at some point we get because of the randomness at least one point that invalidates the completely separable property. This might not be feasible or too expensive.\n",
    "\n",
    "A better way would be to add a regularization term. However in this case it seems that this is nto enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls = LogisticRegression(penalty='l2',C=1e-15)\n",
    "fit = ls.fit(Z,y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plot_decision_surface(Z,fit,ax)\n",
    "plt.scatter(Z[:,0], Z[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
